import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient

def extract_urls_to_mongodb(url, depth=1, max_depth=3, mongo_url='mongodb://localhost:27017/', db_name='urls_database', collection_name='urls'):
    # Connect to MongoDB
    client = MongoClient(mongo_url)
    db = client[db_name]
    collection = db[collection_name]

    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content using BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')

        # Find all anchor tags (a) with an href attribute
        links = soup.find_all('a', href=True)

        for link in links:
            url_to_store = link['href']

            # Check if the URL starts with 'http://' or 'https://'
            if url_to_store.startswith(('http://', 'https://')):
                # Print the URL to the console
                print(url_to_store)

                # Insert the URL into MongoDB
                collection.insert_one({'url': url_to_store})

                # Check if depth is within the limit
                if depth < max_depth:
                    # Recursively call extract_urls_to_mongodb with the new URL
                    extract_urls_to_mongodb(url_to_store, depth=depth + 1, max_depth=max_depth, mongo_url=mongo_url, db_name=db_name, collection_name=collection_name)

        print("URLs saved to MongoDB")
    else:
        print(f"Error: {response.status_code}")

# Example usage
url_to_scrape = "https://sis.kalasalingam.ac.in/login"
max_depth_value = 1
extract_urls_to_mongodb(url_to_scrape, max_depth=max_depth_value)
